import ollama
import streamlit as st

from openai import OpenAI

st.set_page_config(page_title="Ollama + Streamlit Demo",
                   page_icon="ü¶ô",
                   layout="wide",
                   initial_sidebar_state="collapsed"
                   )


def icon(emoji: str):
    """
    Shows an emoji as a Notion-style page icon.

    :param emoji: The emoji to display.

    Returns:
        None
    """

    st.write(
        f'<span style="font-size: 78px; line-height: 1">{emoji}</span>',
        unsafe_allow_html=True,
    )


def extract_model_names(models_info: list) -> tuple:
    """
    Extracts the model names from the models information.

    :param models_info: A dictionary containing the models' information.

    Return: 
        A tuple containing the model names.
    """

    return tuple(model['name'] for model in models_info['models'])


def main():
    """
    The main function that runs the application.
    """

    icon("üë®‚Äçüíª :rainbow[Ollama Ôπ¢ Streamlit UI Demo]")
    st.info(
        "Run models locally using Ollama and a Streamlit UI",
        icon="üí°"
    )

    client = OpenAI(
        base_url='http://localhost:11434/v1',
        api_key='ollama',  # required, but unused
    )

    models_info = ollama.list()
    available_models = extract_model_names(models_info)

    if available_models:
        selected_model = st.selectbox('Pick a model available locally on your system ‚Üì',
                                      available_models
                                      )

        st.divider()

    else:
        st.warning("You have not pulled any model from Ollama yet!", icon="‚ö†Ô∏è")
        st.stop()

    message_container = st.container(height=500, border=True)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with message_container.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Enter a prompt here..."):
        try:
            st.session_state.messages.append(
                {"role": "user", "content": prompt})

            message_container.chat_message("user").markdown(prompt)

            with message_container.chat_message("assistant"):
                stream = client.chat.completions.create(
                    model=selected_model,
                    messages=[
                        {"role": m["role"], "content": m["content"]}
                        for m in st.session_state.messages
                    ],
                    stream=True,
                )
                # stream response
                response = st.write_stream(stream)
            st.session_state.messages.append(
                {"role": "assistant", "content": response})

        except Exception as e:
            st.error(e, icon="‚õîÔ∏è")


if __name__ == "__main__":
    main()
